{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e779e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "sys.path.append(os.path.join(os.getcwd().replace(\"notebooks/pruning_quantization\", \"\")))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from config import Config\n",
    "from data.dataset import NetworkDataset, load_datasets\n",
    "from model_config import MLP_Models, LSTM_Models, GRU_Models, CNN_models\n",
    "import copy\n",
    "import torch.ao.quantization as quant\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fef7123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "conf = Config()\n",
    "load_model = CNN_models()\n",
    "#load_model = MLP_Models()\n",
    "model_conf = load_model.cnn_4\n",
    "model = load_model.get_model(model_conf)\n",
    "print(conf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0dbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_datasets(conf.datasets, model_type=load_model.type)\n",
    "\n",
    "# create train, val and test dataloaders\n",
    "train_dataset = NetworkDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, conf.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = NetworkDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, conf.batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = NetworkDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, conf.batch_size)\n",
    "\n",
    "train_dataset_no_aug = copy.deepcopy(train_dataset)\n",
    "calibration_loader = DataLoader(train_dataset_no_aug, batch_size=conf.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6d73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    #model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)     # FP32 input\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e978fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Macro-F1 score: 0.59, Micro-F1 score: 0.77, Macro ROC AUC score: 0.97, Train loss: 0.409, Val loss: 0.229\n",
      "Epoch: 2/10, Macro-F1 score: 0.65, Micro-F1 score: 0.80, Macro ROC AUC score: 0.97, Train loss: 0.223, Val loss: 0.202\n",
      "Epoch: 3/10, Macro-F1 score: 0.68, Micro-F1 score: 0.81, Macro ROC AUC score: 0.97, Train loss: 0.199, Val loss: 0.183\n",
      "Epoch: 4/10, Macro-F1 score: 0.69, Micro-F1 score: 0.81, Macro ROC AUC score: 0.97, Train loss: 0.186, Val loss: 0.178\n",
      "Epoch: 5/10, Macro-F1 score: 0.73, Micro-F1 score: 0.89, Macro ROC AUC score: 0.98, Train loss: 0.154, Val loss: 0.127\n",
      "Epoch: 6/10, Macro-F1 score: 0.75, Micro-F1 score: 0.90, Macro ROC AUC score: 0.98, Train loss: 0.123, Val loss: 0.119\n",
      "Epoch: 7/10, Macro-F1 score: 0.75, Micro-F1 score: 0.90, Macro ROC AUC score: 0.98, Train loss: 0.114, Val loss: 0.119\n",
      "Epoch: 8/10, Macro-F1 score: 0.76, Micro-F1 score: 0.90, Macro ROC AUC score: 0.99, Train loss: 0.106, Val loss: 0.107\n",
      "Epoch: 9/10, Macro-F1 score: 0.76, Micro-F1 score: 0.90, Macro ROC AUC score: 0.99, Train loss: 0.101, Val loss: 0.100\n",
      "Epoch: 10/10, Macro-F1 score: 0.78, Micro-F1 score: 0.91, Macro ROC AUC score: 0.99, Train loss: 0.097, Val loss: 0.096\n"
     ]
    }
   ],
   "source": [
    "train_metrics, train_loss, val_loss = model.train(train_loader, val_loader, conf.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0628a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (ln1): L2ByteNorm()\n",
       "  (conv): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1, 32, kernel_size=(23,), stride=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(32, 64, kernel_size=(19,), stride=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(15,), stride=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(128, 192, kernel_size=(11,), stride=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Sequential(\n",
       "      (0): Conv1d(192, 256, kernel_size=(7,), stride=(1,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (ln2): LayerNorm((3328,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=3328, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (output): Linear(in_features=256, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_model = model.model\n",
    "fp32_model.cpu()\n",
    "fp32_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead5f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "qconfig = quant.get_default_qconfig(\"fbgemm\")  # x86\n",
    "# qconfig = quant.get_default_qconfig(\"qnnpack\")  # ARM\n",
    "\n",
    "# 1. Copy FP32 model\n",
    "model_to_quantize = copy.deepcopy(fp32_model)\n",
    "model_to_quantize.cpu()\n",
    "model_to_quantize.eval()\n",
    "\n",
    "# .set_module_name(\"ln2\", None)     # LayerNorm FP32\n",
    "qconfig_mapping = (\n",
    "    quant.QConfigMapping()\n",
    "    .set_global(qconfig)\n",
    "    .set_module_name(\"ln1\", None)     # LayerNorm FP32\n",
    "    .set_module_name(\"ln2\", None)     # LayerNorm FP32\n",
    "    .set_module_name(\"ln3\", None)     # LayerNorm FP32\n",
    "    .set_module_name(\"output\", None)  # Linear FP32\n",
    ")\n",
    "\n",
    "example_input = torch.randn(1, 1, 513)\n",
    "\n",
    "prepared = prepare_fx(\n",
    "    model_to_quantize,\n",
    "    qconfig_mapping,\n",
    "    example_inputs=(example_input,)\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, _) in enumerate(calibration_loader):\n",
    "        prepared(x)\n",
    "        if i > 20:\n",
    "            break\n",
    "\n",
    "int8_model = convert_fx(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a91812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (conv): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedConv1d(1, 32, kernel_size=(23,), stride=(1,), scale=1.2880123853683472, zero_point=66)\n",
      "      (1): QuantizedLeakyReLU(negative_slope=0.01)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Module(\n",
      "      (0): QuantizedConv1d(32, 64, kernel_size=(19,), stride=(1,), scale=0.6548764705657959, zero_point=69)\n",
      "      (1): QuantizedLeakyReLU(negative_slope=0.01)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Module(\n",
      "      (0): QuantizedConv1d(64, 128, kernel_size=(15,), stride=(1,), scale=0.3296360373497009, zero_point=78)\n",
      "      (1): QuantizedLeakyReLU(negative_slope=0.01)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Module(\n",
      "      (0): QuantizedConv1d(128, 192, kernel_size=(11,), stride=(1,), scale=0.1940612941980362, zero_point=78)\n",
      "      (1): QuantizedLeakyReLU(negative_slope=0.01)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Module(\n",
      "      (0): QuantizedConv1d(192, 256, kernel_size=(7,), stride=(1,), scale=0.07119409739971161, zero_point=86)\n",
      "      (1): QuantizedLeakyReLU(negative_slope=0.01)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Module(\n",
      "    (0): Module(\n",
      "      (0): QuantizedLinearReLU(in_features=3328, out_features=512, scale=0.023212898522615433, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): QuantizedLinearReLU(in_features=512, out_features=256, scale=0.016604948788881302, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "      (2): QuantizedDropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (output): Linear(in_features=256, out_features=24, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    getattr_1 = x.shape\n",
      "    getitem = getattr_1[-1];  getattr_1 = None\n",
      "    sub = getitem - 13;  getitem = None\n",
      "    split = x.split([13, sub], dim = 2);  x = sub = None\n",
      "    getitem_1 = split[0]\n",
      "    getitem_2 = split[1];  split = None\n",
      "    normalize = torch.nn.functional.normalize(getitem_1, p = 2.0, dim = 2, eps = 1e-12, out = None);  getitem_1 = None\n",
      "    normalize_1 = torch.nn.functional.normalize(getitem_2, p = 2.0, dim = 2, eps = 1e-12, out = None);  getitem_2 = None\n",
      "    cat = torch.cat((normalize, normalize_1), dim = 2);  normalize = normalize_1 = None\n",
      "    mul = cat * 255.0;  cat = None\n",
      "    conv_0_0_input_scale_0 = self.conv_0_0_input_scale_0\n",
      "    conv_0_0_input_zero_point_0 = self.conv_0_0_input_zero_point_0\n",
      "    quantize_per_tensor = torch.quantize_per_tensor(mul, conv_0_0_input_scale_0, conv_0_0_input_zero_point_0, torch.quint8);  mul = conv_0_0_input_scale_0 = conv_0_0_input_zero_point_0 = None\n",
      "    conv_0_0 = getattr(getattr(self.conv, \"0\"), \"0\")(quantize_per_tensor);  quantize_per_tensor = None\n",
      "    conv_0_1 = getattr(getattr(self.conv, \"0\"), \"1\")(conv_0_0);  conv_0_0 = None\n",
      "    conv_0_2 = getattr(getattr(self.conv, \"0\"), \"2\")(conv_0_1);  conv_0_1 = None\n",
      "    conv_1 = getattr(self.conv, \"1\")(conv_0_2);  conv_0_2 = None\n",
      "    conv_2_0 = getattr(getattr(self.conv, \"2\"), \"0\")(conv_1);  conv_1 = None\n",
      "    conv_2_1 = getattr(getattr(self.conv, \"2\"), \"1\")(conv_2_0);  conv_2_0 = None\n",
      "    conv_2_2 = getattr(getattr(self.conv, \"2\"), \"2\")(conv_2_1);  conv_2_1 = None\n",
      "    conv_3 = getattr(self.conv, \"3\")(conv_2_2);  conv_2_2 = None\n",
      "    conv_4_0 = getattr(getattr(self.conv, \"4\"), \"0\")(conv_3);  conv_3 = None\n",
      "    conv_4_1 = getattr(getattr(self.conv, \"4\"), \"1\")(conv_4_0);  conv_4_0 = None\n",
      "    conv_4_2 = getattr(getattr(self.conv, \"4\"), \"2\")(conv_4_1);  conv_4_1 = None\n",
      "    conv_5 = getattr(self.conv, \"5\")(conv_4_2);  conv_4_2 = None\n",
      "    conv_6_0 = getattr(getattr(self.conv, \"6\"), \"0\")(conv_5);  conv_5 = None\n",
      "    conv_6_1 = getattr(getattr(self.conv, \"6\"), \"1\")(conv_6_0);  conv_6_0 = None\n",
      "    conv_6_2 = getattr(getattr(self.conv, \"6\"), \"2\")(conv_6_1);  conv_6_1 = None\n",
      "    conv_7 = getattr(self.conv, \"7\")(conv_6_2);  conv_6_2 = None\n",
      "    conv_8_0 = getattr(getattr(self.conv, \"8\"), \"0\")(conv_7);  conv_7 = None\n",
      "    conv_8_1 = getattr(getattr(self.conv, \"8\"), \"1\")(conv_8_0);  conv_8_0 = None\n",
      "    conv_8_2 = getattr(getattr(self.conv, \"8\"), \"2\")(conv_8_1);  conv_8_1 = None\n",
      "    dequantize_19 = conv_8_2.dequantize();  conv_8_2 = None\n",
      "    flatten = self.flatten(dequantize_19);  dequantize_19 = None\n",
      "    flatten_scale_0 = self.flatten_scale_0\n",
      "    flatten_zero_point_0 = self.flatten_zero_point_0\n",
      "    quantize_per_tensor_20 = torch.quantize_per_tensor(flatten, flatten_scale_0, flatten_zero_point_0, torch.quint8);  flatten = flatten_scale_0 = flatten_zero_point_0 = None\n",
      "    linear_0_0 = getattr(getattr(self.linear, \"0\"), \"0\")(quantize_per_tensor_20);  quantize_per_tensor_20 = None\n",
      "    linear_0_2 = getattr(getattr(self.linear, \"0\"), \"2\")(linear_0_0);  linear_0_0 = None\n",
      "    linear_1_0 = getattr(getattr(self.linear, \"1\"), \"0\")(linear_0_2);  linear_0_2 = None\n",
      "    linear_1_2 = getattr(getattr(self.linear, \"1\"), \"2\")(linear_1_0);  linear_1_0 = None\n",
      "    dequantize_24 = linear_1_2.dequantize();  linear_1_2 = None\n",
      "    ln3 = self.ln3(dequantize_24);  dequantize_24 = None\n",
      "    output = self.output(ln3);  ln3 = None\n",
      "    return output\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(int8_model)\n",
    "#print(int8_model.fc1.weight().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963630e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 accuracy: 0.9069\n",
      "INT8 accuracy: 0.8931\n",
      "Drop: 0.0138\n"
     ]
    }
   ],
   "source": [
    "#Verify Quantization\n",
    "fp32_acc = evaluate(fp32_model, test_loader)\n",
    "int8_acc = evaluate(int8_model, test_loader)\n",
    "\n",
    "print(f\"FP32 accuracy: {fp32_acc:.4f}\")\n",
    "print(f\"INT8 accuracy: {int8_acc:.4f}\")\n",
    "print(f\"Drop: {fp32_acc - int8_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee31a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  0, 14,  0, 12, 10,  5, 15, 10,  5])\n",
      "tensor([ 0,  0, 14,  0, 12, 10,  5, 15, 10,  5])\n",
      "tensor([ 0,  0, 14,  0, 12, 10,  5, 15, 10,  5])\n"
     ]
    }
   ],
   "source": [
    "data, labels = next(iter(test_loader))\n",
    "q_pred = int8_model(data).argmax(dim=1)\n",
    "pred = fp32_model(data).argmax(dim=1)\n",
    "\n",
    "print(labels[:10])\n",
    "print(q_pred[:10])\n",
    "print(pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a9f8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nint8_model = quant.quantize_dynamic(\\n    fp32_model,\\n    {nn.Linear},\\n    dtype=torch.qint8\\n)\\n\\nint8_model.fc2 = fp32_model.fc2  # overwrite last layer with FP32\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "int8_model = quant.quantize_dynamic(\n",
    "    fp32_model,\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "int8_model.fc2 = fp32_model.fc2  # overwrite last layer with FP32\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
