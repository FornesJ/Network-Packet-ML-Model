{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e1dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(os.path.join(os.getcwd().replace(\"split_models\", \"\")))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from parse_dataset import NetworkDataset\n",
    "from model import SplitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"model\": \"lstm\",\n",
    "    \"train_dpu\": False,\n",
    "    \"train_host\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6275b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2cda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([356334, 513, 1])\n",
      "torch.Size([356334])\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(os.getcwd().replace(\"split_models\", \"\"), \"datasets\")\n",
    "\n",
    "# load label_dict\n",
    "json_file = os.path.join(dataset_path, \"label_index.json\")\n",
    "with open(json_file, 'r') as file:\n",
    "    label_dict = json.load(file)\n",
    "\n",
    "# load train, val and test datasets\n",
    "train_dataset_file = os.path.join(dataset_path, \"train_dataset.pt\")\n",
    "X_train, y_train = torch.load(train_dataset_file)\n",
    "\n",
    "val_dataset_file = os.path.join(dataset_path, \"val_dataset.pt\")\n",
    "X_val, y_val = torch.load(val_dataset_file)\n",
    "\n",
    "test_dataset_file = os.path.join(dataset_path, \"test_dataset.pt\")\n",
    "X_test, y_test = torch.load(test_dataset_file)\n",
    "\n",
    "if conf[\"model\"] != \"mlp\" and conf[\"model\"] != \"light_mlp\":\n",
    "    X_train, X_val, X_test = X_train.unsqueeze(-1), X_val.unsqueeze(-1), X_test.unsqueeze(-1)\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# create train, val and test datasets\n",
    "train_dataset = NetworkDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=conf[\"batch_size\"], shuffle=True)\n",
    "\n",
    "val_dataset = NetworkDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=conf[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_dataset = NetworkDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=conf[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4980da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitLSTM(nn.Module):\n",
    "    def __init__(self, i_size, h_size, num_layers):\n",
    "        super(SplitLSTM, self).__init__()\n",
    "        self.i_size = i_size\n",
    "        self.h_size = h_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = 0.0\n",
    "\n",
    "        if self.num_layers > 1:\n",
    "            self.dropout = 0.15\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=i_size, \n",
    "                            hidden_size=h_size, \n",
    "                            num_layers=self.num_layers, \n",
    "                            batch_first=True,\n",
    "                            dropout=self.dropout, \n",
    "                            bidirectional=True, \n",
    "                            device=device)\n",
    "        self.bn1 = nn.BatchNorm1d(2 * h_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*self.h_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(128, 24)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(2*self.num_layers, x.shape[0], self.h_size).to(device)\n",
    "            c0 = torch.zeros(2*self.num_layers, x.shape[0], self.h_size).to(device)\n",
    "        \n",
    "        output, (h0, c0) = self.lstm(x, (h0, c0))  # output: [B, T, 2*h_size]\n",
    "\n",
    "        # take last layer's hidden state (both directions)\n",
    "        # h0 shape: [num_layers*2, B, size]\n",
    "        h_last = h0.view(self.num_layers, 2, x.shape[0], self.h_size)[-1]  # [2, B, h_size]\n",
    "        h_last = torch.cat((h_last[0], h_last[1]), dim=1)  # [B, 2*h_size]\n",
    "\n",
    "        # apply BN + FC\n",
    "        h_last = self.bn1(h_last)         # [B, 2*h_size] â†’ batch norm\n",
    "        logits = h_last.unsqueeze(1)\n",
    "        out = self.fc(h_last)\n",
    "\n",
    "        return logits, out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397e1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.1, alpha=0.9, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf96d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "dpu_lstm_model = SplitLSTM(i_size=1, h_size=64, num_layers=1).to(device=device)\n",
    "dpu_lstm_criterion = FocalLoss()\n",
    "dpu_lstm_optimizer = torch.optim.AdamW(dpu_lstm_model.parameters(), conf[\"learning_rate\"], weight_decay=0.01)\n",
    "dpu_lstm_scheduler = torch.optim.lr_scheduler.ExponentialLR(dpu_lstm_optimizer, 0.9)\n",
    "dpu_lstm = SplitModel(dpu_lstm_model, dpu_lstm_criterion, dpu_lstm_optimizer, dpu_lstm_scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65ae422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Accuracy: 85.65%, Train loss: 0.7184, Val loss: 0.2934\n",
      "Epoch: 2/10, Accuracy: 92.59%, Train loss: 0.2262, Val loss: 0.1494\n",
      "Epoch: 3/10, Accuracy: 93.83%, Train loss: 0.1385, Val loss: 0.1077\n",
      "Epoch: 4/10, Accuracy: 94.70%, Train loss: 0.1083, Val loss: 0.0885\n",
      "Epoch: 5/10, Accuracy: 95.12%, Train loss: 0.0922, Val loss: 0.0778\n",
      "Epoch: 6/10, Accuracy: 95.47%, Train loss: 0.0827, Val loss: 0.0702\n",
      "Epoch: 7/10, Accuracy: 95.67%, Train loss: 0.0757, Val loss: 0.0644\n",
      "Epoch: 8/10, Accuracy: 95.78%, Train loss: 0.0696, Val loss: 0.0605\n",
      "Epoch: 9/10, Accuracy: 95.96%, Train loss: 0.0659, Val loss: 0.0575\n",
      "Epoch: 10/10, Accuracy: 95.98%, Train loss: 0.0629, Val loss: 0.0560\n",
      "Checkpoint saved at /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/dpu_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "dpu_path = os.path.join(os.getcwd().replace(\"split_models\", \"\"), \"checkpoint\", \"dpu_lstm_model.pth\")\n",
    "if conf[\"train_dpu\"]:\n",
    "    acc, train_loss, val_loss = dpu_lstm.train(train_loader, val_loader, conf[\"epochs\"])\n",
    "    dpu_lstm.save(dpu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bad4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/dpu_lstm_model.pth!\n"
     ]
    }
   ],
   "source": [
    "# load model: \n",
    "dpu_lstm.load(dpu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a963bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.98%, Val loss: 0.0560\n"
     ]
    }
   ],
   "source": [
    "val_loss, acc = dpu_lstm.evaluate(val_loader)\n",
    "print(f\"Accuracy: {100*acc:.2f}%, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5962ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "host_lstm_model = SplitLSTM(i_size=2*64, h_size=64, num_layers=3).to(device=device)\n",
    "host_lstm_criterion = FocalLoss()\n",
    "host_lstm_optimizer = torch.optim.AdamW(host_lstm_model.parameters(), lr=conf[\"learning_rate\"], weight_decay=0.01)\n",
    "host_lstm_scheduler = torch.optim.lr_scheduler.ExponentialLR(host_lstm_optimizer, 0.9)\n",
    "host_lstm = SplitModel(host_lstm_model, host_lstm_criterion, host_lstm_optimizer, host_lstm_scheduler, device, dpu_model=dpu_lstm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2325323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Accuracy: 93.40%, Train loss: 0.4761, Val loss: 0.1320\n",
      "Epoch: 2/10, Accuracy: 94.83%, Train loss: 0.1225, Val loss: 0.0830\n",
      "Epoch: 3/10, Accuracy: 95.54%, Train loss: 0.0887, Val loss: 0.0674\n",
      "Epoch: 4/10, Accuracy: 95.89%, Train loss: 0.0755, Val loss: 0.0599\n",
      "Epoch: 5/10, Accuracy: 96.04%, Train loss: 0.0681, Val loss: 0.0550\n",
      "Epoch: 6/10, Accuracy: 96.18%, Train loss: 0.0634, Val loss: 0.0518\n",
      "Epoch: 7/10, Accuracy: 96.28%, Train loss: 0.0596, Val loss: 0.0495\n",
      "Epoch: 8/10, Accuracy: 96.36%, Train loss: 0.0567, Val loss: 0.0476\n",
      "Epoch: 9/10, Accuracy: 96.44%, Train loss: 0.0545, Val loss: 0.0463\n",
      "Epoch: 10/10, Accuracy: 96.56%, Train loss: 0.0529, Val loss: 0.0448\n",
      "Checkpoint saved at /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/host_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "host_path = os.path.join(os.getcwd().replace(\"split_models\", \"\"), \"checkpoint\", \"host_lstm_model.pth\")\n",
    "if conf[\"train_host\"]:\n",
    "    acc, train_loss, val_loss = host_lstm.train(train_loader, val_loader, conf[\"epochs\"])\n",
    "    host_lstm.save(host_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb64f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/host_lstm_model.pth!\n"
     ]
    }
   ],
   "source": [
    "# load model:\n",
    "host_lstm.load(host_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f67c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.56%, Val loss: 0.0448\n"
     ]
    }
   ],
   "source": [
    "val_loss, acc = host_lstm.evaluate(val_loader)\n",
    "print(f\"Accuracy: {100*acc:.2f}%, Val loss: {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
