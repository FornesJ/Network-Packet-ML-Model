{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9072e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd().replace(\"large_models\", \"\")))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profiler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from parse_dataset import NetworkDataset, parse_dataset, split_datasets\n",
    "from model import Model\n",
    "from load_models import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94d81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"batch_size\": 516,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"load_model\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adff5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373f938b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packet_dat,attack_cat\n",
      "\n",
      "torch.Size([356334, 513, 1])\n",
      "torch.Size([356334])\n"
     ]
    }
   ],
   "source": [
    "# extract dataset from csv file\n",
    "network_data_file = os.path.join(os.getcwd().replace(\"large_models\", \"\"), \"datasets\", \"network_packet_data_test.csv\")\n",
    "\n",
    "data, labels, label_dict = parse_dataset(network_data_file)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_datasets(data, labels)\n",
    "\n",
    "X_train, X_val, X_test = X_train.unsqueeze(-1), X_val.unsqueeze(-1), X_test.unsqueeze(-1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# create train, val and test datasets\n",
    "train_dataset = NetworkDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=conf[\"batch_size\"], shuffle=True)\n",
    "\n",
    "val_dataset = NetworkDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=conf[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_dataset = NetworkDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=conf[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fc581b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary dataset\n",
    "def binary_dataset(labels, label_dict):\n",
    "    binary_labels = []\n",
    "    label_names = list(label_dict.keys())\n",
    "    for label in labels:\n",
    "        if label_names[label] == \"Normal\":\n",
    "            binary_labels.append(0)\n",
    "        else:\n",
    "            binary_labels.append(1)\n",
    "    y_binary = torch.tensor(binary_labels, dtype=torch.float).to(device)\n",
    "    return y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2aefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GRU model:\n",
    "class SPLITGRU(nn.Module):\n",
    "    def __init__(self, i_size, h_size, binary=False):\n",
    "        super(SPLITGRU, self).__init__()\n",
    "        self.i_size = i_size\n",
    "        self.h_size = h_size\n",
    "        self.binary = binary\n",
    "        self.gru1 = nn.GRU(input_size=i_size, hidden_size=h_size, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True, device=device)\n",
    "        self.bn1 = nn.BatchNorm1d(2 * h_size)\n",
    "\n",
    "        self.fc_binary = nn.Sequential(\n",
    "            nn.Linear(2*self.h_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.fc_class = nn.Sequential(\n",
    "            nn.Linear(2*self.h_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(128, 24)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(4, x.shape[0], self.h_size).to(device)\n",
    "\n",
    "        output, h0 = self.gru1(x, h0)  # output: [B, T, 2*h_size]\n",
    "\n",
    "        # take last layer's hidden state (both directions)\n",
    "        # h0 shape: [num_layers*2, B, size]\n",
    "        h_last = h0.view(2, 2, x.shape[0], self.h_size)[-1]  # [2, B, h_size]\n",
    "        h_last = torch.cat((h_last[0], h_last[1]), dim=1)  # [B, 2*h_size]\n",
    "\n",
    "        # apply BN + FC\n",
    "        logits = self.bn1(h_last)  # [B, 2*h_size] â†’ batch norm\n",
    "        if self.binary:\n",
    "            out = self.fc_binary(logits) # [B, 1]\n",
    "        else:\n",
    "            out = self.fc_class(logits) # [B, 24]\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.1, alpha=0.9, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bed5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu_model = SPLITGRU(X_train.shape[2], 64, binary=True).to(device)\n",
    "dpu_criterion = nn.BCELoss()\n",
    "dpu_optimizer = torch.optim.AdamW(dpu_model.parameters(), lr=conf[\"learning_rate\"], weight_decay=0.01)\n",
    "dpu_scheduler = torch.optim.lr_scheduler.ExponentialLR(dpu_optimizer, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20a10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_model = SPLITGRU(128, 64).to(device)\n",
    "host_criterion = FocalLoss()\n",
    "host_optimizer = torch.optim.AdamW(host_model.parameters(), lr=conf[\"learning_rate\"], weight_decay=0.01)\n",
    "host_scheduler = torch.optim.lr_scheduler.ExponentialLR(host_optimizer, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f01a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_loader):\n",
    "        y_true, y_pred = [], []\n",
    "        y_bin_true, y_bin_pred = [], []\n",
    "\n",
    "        dpu_model.eval()\n",
    "        host_model.eval()\n",
    "        for (data, labels) in val_loader:\n",
    "            if not data.is_cuda or not labels.is_cuda:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            bin_labels = binary_dataset(labels, label_dict)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                bin_pred, logits = dpu_model(data)\n",
    "                logits = logits.detach()  # break the graph here\n",
    "                logits = logits.unsqueeze(1)\n",
    "                pred, _ = host_model(logits)\n",
    "                bin_pred = torch.squeeze(bin_pred)\n",
    "            \n",
    "            y_true.append(labels)\n",
    "            y_pred.append(pred)\n",
    "            y_bin_true.append(bin_labels)\n",
    "            y_bin_pred.append(bin_pred)\n",
    "\n",
    "        # concat predictions and targets\n",
    "        y_true, y_pred = torch.cat(y_true, dim=0), torch.cat(y_pred, dim=0)\n",
    "        y_bin_true, y_bin_pred = torch.cat(y_bin_true, dim=0), torch.cat(y_bin_pred, dim=0)\n",
    "\n",
    "        # calculate loss\n",
    "        bin_loss = dpu_criterion(y_bin_pred, y_bin_true)\n",
    "        class_loss = host_criterion(y_pred, y_true)\n",
    "\n",
    "        # evaluate accuracy\n",
    "        class_acc = (y_pred.argmax(dim=1) == y_true).float().mean()\n",
    "        bin_acc = (y_bin_pred.round() == y_bin_true).float().mean()\n",
    "\n",
    "        return class_loss, class_acc, bin_loss, bin_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98bae574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, epochs):\n",
    "    class_acc_list = []\n",
    "    bin_acc_list = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        dpu_model.train()\n",
    "        host_model.train()\n",
    "        running_class_loss = 0.0\n",
    "        running_bin_loss = 0.0\n",
    "\n",
    "        # train model\n",
    "        for (data, labels) in train_loader:\n",
    "            if not data.is_cuda or not labels.is_cuda:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            bin_labels = binary_dataset(labels, label_dict)\n",
    "            \n",
    "            bin_pred, logits = dpu_model(data)\n",
    "            logits = logits.detach()  # break the graph here\n",
    "            logits = logits.unsqueeze(1)\n",
    "            pred, _ = host_model(logits)\n",
    "            bin_pred = torch.squeeze(bin_pred)\n",
    "\n",
    "            # calculate loss\n",
    "            bin_loss = dpu_criterion(bin_pred, bin_labels)\n",
    "            class_loss = host_criterion(pred, labels)\n",
    "            running_bin_loss += bin_loss.item() * data.size(0)\n",
    "            running_class_loss += class_loss.item() * logits.size(0)\n",
    "\n",
    "            dpu_optimizer.zero_grad()\n",
    "            bin_loss.backward()\n",
    "            dpu_optimizer.step()\n",
    "\n",
    "            host_optimizer.zero_grad()\n",
    "            class_loss.backward()\n",
    "            host_optimizer.step()\n",
    "        \n",
    "        # evaluate model\n",
    "        val_class_loss, class_acc, val_bin_loss, bin_acc = evaluate(val_loader)\n",
    "        #train_bin_loss = running_bin_loss / (train_loader.__len__() * train_loader.batch_size)\n",
    "        #train_class_loss = running_class_loss / (train_loader.__len__() * logits.size(0))\n",
    "        dpu_scheduler.step()\n",
    "        host_scheduler.step()\n",
    "\n",
    "        bin_acc_list.append(bin_acc)\n",
    "        class_acc_list.append(class_acc)\n",
    "    \n",
    "        print(f\"Epoch: {epoch}/{epochs}, DPU model accuracy: {100*bin_acc:.2f}%, Host model accuracy: {100*class_acc:.2f}%\")\n",
    "\n",
    "    return bin_acc, class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45d7ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, DPU model accuracy: 98.74%, Host model accuracy: 82.23%\n",
      "Epoch: 2/10, DPU model accuracy: 99.28%, Host model accuracy: 83.98%\n",
      "Epoch: 3/10, DPU model accuracy: 99.39%, Host model accuracy: 87.27%\n",
      "Epoch: 4/10, DPU model accuracy: 99.44%, Host model accuracy: 88.42%\n",
      "Epoch: 5/10, DPU model accuracy: 99.48%, Host model accuracy: 89.31%\n",
      "Epoch: 6/10, DPU model accuracy: 99.52%, Host model accuracy: 92.33%\n",
      "Epoch: 7/10, DPU model accuracy: 99.57%, Host model accuracy: 91.81%\n",
      "Epoch: 8/10, DPU model accuracy: 99.59%, Host model accuracy: 92.31%\n",
      "Epoch: 9/10, DPU model accuracy: 99.61%, Host model accuracy: 93.63%\n",
      "Epoch: 10/10, DPU model accuracy: 99.60%, Host model accuracy: 92.89%\n"
     ]
    }
   ],
   "source": [
    "# train dpu model\n",
    "bin_acc, class_acc = train(train_loader, val_loader, conf[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "458d334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(checkpoint_path, model, optimizer, scheduler):\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17f3a79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/dpu_split_model.pth\n",
      "Checkpoint saved at /home/jorgetf/testmodel/Network-Packet-ML-Model/checkpoint/host_split_model.pth\n"
     ]
    }
   ],
   "source": [
    "# save models:\n",
    "dpu_checkpoint_path = os.path.join(os.getcwd().replace(\"large_models\", \"\"), \"checkpoint\", \"dpu_split_model.pth\")\n",
    "host_checkpoint_path = os.path.join(os.getcwd().replace(\"large_models\", \"\"), \"checkpoint\", \"host_split_model.pth\")\n",
    "\n",
    "save(dpu_checkpoint_path, dpu_model, dpu_optimizer, dpu_scheduler)\n",
    "save(host_checkpoint_path, host_model, host_optimizer, host_scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
